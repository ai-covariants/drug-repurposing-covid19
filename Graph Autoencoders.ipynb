{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2H1QVEAgeQI",
        "outputId": "50bcf127-73d5-406e-86ce-25b5dd91b707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "import warnings\n",
        "import keras\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "42Qkzr6ziZAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
        "    \"\"\"Create a weight variable with Glorot & Bengio (AISTATS 2010)\n",
        "    initialization.\n",
        "    \"\"\"\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = tf.random_uniform([input_dim, output_dim], minval=-init_range,\n",
        "                                maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)"
      ],
      "metadata": {
        "id": "D6d6lZYVdTJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs\n",
        "    \"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
        "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
        "    \"\"\"\n",
        "    noise_shape = [num_nonzero_elems]\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.issparse = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            outputs = self._call(inputs)\n",
        "            return outputs\n",
        "\n",
        "class GraphConvolution(Layer):\n",
        "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n",
        "        self.dropout = dropout\n",
        "        self.adj = adj\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "        x = tf.nn.dropout(x, 1-self.dropout)\n",
        "        x = tf.matmul(x, self.vars['weights'])\n",
        "        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
        "        outputs = self.act(x)\n",
        "        return outputs\n",
        "class GraphConvolutionSparse(Layer):\n",
        "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, adj, features_nonzero, dropout=0., act=tf.nn.relu, **kwargs):\n",
        "        super(GraphConvolutionSparse, self).__init__(**kwargs)\n",
        "        with tf.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n",
        "        self.dropout = dropout\n",
        "        self.adj = adj\n",
        "        self.act = act\n",
        "        self.issparse = True\n",
        "        self.features_nonzero = features_nonzero\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "        x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
        "        x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
        "        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
        "        outputs = self.act(x)\n",
        "        return outputs\n",
        "class InnerProductDecoder(Layer):\n",
        "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
        "    def __init__(self, input_dim, dropout=0., act=tf.nn.sigmoid, **kwargs):\n",
        "        super(InnerProductDecoder, self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.act = act\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
        "        x = tf.transpose(inputs)\n",
        "        x = tf.matmul(inputs, x)\n",
        "        x = tf.reshape(x, [-1])\n",
        "        outputs = self.act(x)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "PU-sY15ydWcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(object):\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            name = self.__class__.__name__.lower()\n",
        "        self.name = name\n",
        "\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "\n",
        "        self.vars = {}\n",
        "\n",
        "    def _build(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\" Wrapper for _build() \"\"\"\n",
        "        with tf.variable_scope(self.name):\n",
        "            self._build()\n",
        "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
        "        self.vars = {var.name: var for var in variables}\n",
        "\n",
        "    def fit(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "class GCNModelAE(Model):\n",
        "    def __init__(self, placeholders, num_features, features_nonzero, **kwargs):\n",
        "        super(GCNModelAE, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = num_features\n",
        "        self.features_nonzero = features_nonzero\n",
        "        self.adj = placeholders['adj']\n",
        "        self.dropout = placeholders['dropout']\n",
        "        self.build()\n",
        "\n",
        "    def _build(self):\n",
        "        self.hidden1 = GraphConvolutionSparse(input_dim=self.input_dim,\n",
        "                                              output_dim=128,\n",
        "                                              adj=self.adj,\n",
        "                                              features_nonzero=self.features_nonzero,\n",
        "                                              act=tf.nn.relu,\n",
        "                                              dropout=self.dropout,\n",
        "                                              logging=self.logging)(self.inputs)\n",
        "\n",
        "        self.embeddings = GraphConvolution(input_dim=128,\n",
        "                                           output_dim=16,\n",
        "                                           adj=self.adj,\n",
        "                                           act=lambda x: x,\n",
        "                                           dropout=self.dropout,\n",
        "                                           logging=self.logging)(self.hidden1)\n",
        "\n",
        "        self.z_mean = self.embeddings\n",
        "\n",
        "        self.reconstructions = InnerProductDecoder(input_dim=16,\n",
        "                                      act=lambda x: x,\n",
        "                                      logging=self.logging)(self.embeddings)"
      ],
      "metadata": {
        "id": "3wy9xu_7de--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizerAE(object):\n",
        "    def __init__(self, preds, labels, pos_weight, norm):\n",
        "        preds_sub = preds\n",
        "        labels_sub = labels\n",
        "\n",
        "        self.cost = norm * tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.01)  # Adam Optimizer\n",
        "\n",
        "        self.opt_op = self.optimizer.minimize(self.cost)\n",
        "        self.grads_vars = self.optimizer.compute_gradients(self.cost)\n",
        "\n",
        "        self.correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.sigmoid(preds_sub), 0.5), tf.int32),\n",
        "                                           tf.cast(labels_sub, tf.int32))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))"
      ],
      "metadata": {
        "id": "9lULmLcpdoJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    if not sp.isspmatrix_coo(sparse_mx):\n",
        "        sparse_mx = sparse_mx.tocoo()\n",
        "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
        "    values = sparse_mx.data\n",
        "    shape = sparse_mx.shape\n",
        "    return coords, values, shape\n",
        "\n",
        "\n",
        "def preprocess_graph(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    adj_ = adj + sp.eye(adj.shape[0])\n",
        "    rowsum = np.array(adj_.sum(1))\n",
        "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
        "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
        "    # construct feed dictionary\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
        "    feed_dict.update({placeholders['adj_orig']: adj})\n",
        "    return feed_dict\n",
        "\n",
        "\n",
        "def mask_test_edges(adj):\n",
        "    # Function to build test set with 10% positive links\n",
        "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
        "    # TODO: Clean up.\n",
        "\n",
        "    # Remove diagonal elements\n",
        "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
        "    adj.eliminate_zeros()\n",
        "    # Check that diag is zero:\n",
        "    assert np.diag(adj.todense()).sum() == 0\n",
        "\n",
        "    adj_triu = sp.triu(adj)\n",
        "    adj_tuple = sparse_to_tuple(adj_triu)\n",
        "    edges = adj_tuple[0]\n",
        "    edges_all = sparse_to_tuple(adj)[0]\n",
        "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
        "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
        "\n",
        "    all_edge_idx = list(range(edges.shape[0]))\n",
        "    np.random.shuffle(all_edge_idx)\n",
        "    val_edge_idx = all_edge_idx[:num_val]\n",
        "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
        "    test_edges = edges[test_edge_idx]\n",
        "    val_edges = edges[val_edge_idx]\n",
        "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
        "\n",
        "    def ismember(a, b, tol=5):\n",
        "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
        "        return np.any(rows_close)\n",
        "\n",
        "    test_edges_false = []\n",
        "    while len(test_edges_false) < len(test_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], edges_all):\n",
        "            continue\n",
        "        if test_edges_false:\n",
        "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
        "                continue\n",
        "        test_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "    val_edges_false = []\n",
        "    while len(val_edges_false) < len(val_edges):\n",
        "        idx_i = np.random.randint(0, adj.shape[0])\n",
        "        idx_j = np.random.randint(0, adj.shape[0])\n",
        "        if idx_i == idx_j:\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], train_edges):\n",
        "            continue\n",
        "        if ismember([idx_j, idx_i], train_edges):\n",
        "            continue\n",
        "        if ismember([idx_i, idx_j], val_edges):\n",
        "            continue\n",
        "        if ismember([idx_j, idx_i], val_edges):\n",
        "            continue\n",
        "        if val_edges_false:\n",
        "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
        "                continue\n",
        "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
        "                continue\n",
        "        val_edges_false.append([idx_i, idx_j])\n",
        "\n",
        "    #assert ~ismember(test_edges_false, edges_all)\n",
        "    #assert ~ismember(val_edges_false, edges_all)\n",
        "    #assert ~ismember(val_edges, train_edges)\n",
        "    #assert ~ismember(test_edges, train_edges)\n",
        "    #assert ~ismember(val_edges, test_edges)\n",
        "\n",
        "    data = np.ones(train_edges.shape[0])\n",
        "\n",
        "    # Re-build adj matrix\n",
        "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
        "    adj_train = adj_train + adj_train.T\n",
        "\n",
        "    # NOTE: these edge lists only contain single direction of edge!\n",
        "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false"
      ],
      "metadata": {
        "id": "QLF6j8X7dtf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "adj = pd.read_csv('DDR-HC-new.csv')\n",
        "features = pd.read_csv('all_drug_data_processed.csv')"
      ],
      "metadata": {
        "id": "HlHWXpDAeKlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature_matrix = feature_matrix.drop('Unnamed: 0',axis=1)\n",
        "features.set_index('Name',inplace=True)\n",
        "adj.set_index('Unnamed: 0', inplace=True)"
      ],
      "metadata": {
        "id": "vXLdyqodjaJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = sp.csr_matrix(features.astype(pd.SparseDtype(\"float64\",0)).sparse.to_coo())\n",
        "adj = sp.csr_matrix(adj.astype(pd.SparseDtype(\"float64\",0)))\n",
        "s = StandardScaler(with_mean=False)\n",
        "features = s.fit_transform(features)"
      ],
      "metadata": {
        "id": "VenC_Ikgjhw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store original adjacency matrix (without diagonal entries) for later\n",
        "adj_orig = adj\n",
        "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
        "adj_orig.eliminate_zeros()\n",
        "\n",
        "#adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
        "#adj = adj_train\n",
        "adj = adj_orig\n",
        "\n",
        "# Some preprocessing\n",
        "adj_norm = preprocess_graph(adj)\n",
        "\n",
        "# Define placeholders\n",
        "placeholders = {\n",
        "    'features': tf.sparse_placeholder(tf.float32),\n",
        "    'adj': tf.sparse_placeholder(tf.float32),\n",
        "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=())\n",
        "}\n",
        "\n",
        "num_nodes = adj.shape[0]\n",
        "\n",
        "features = sparse_to_tuple(features.tocoo())\n",
        "num_features = features[2][1]\n",
        "features_nonzero = features[1].shape[0]"
      ],
      "metadata": {
        "id": "97CStYwKeObQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model = None\n",
        "model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
        "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
        "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
        "\n",
        "# Optimizer\n",
        "with tf.name_scope('optimizer'):\n",
        "  opt = OptimizerAE(preds=model.reconstructions,labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],validate_indices=False), [-1]),pos_weight=pos_weight,norm=norm)"
      ],
      "metadata": {
        "id": "4WOVNBmOeUMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "cost_val = []\n",
        "acc_val = []\n",
        "def get_roc_score(edges_pos, edges_neg, emb=None):\n",
        "    if emb is None:\n",
        "        feed_dict.update({placeholders['dropout']: 0})\n",
        "        emb = sess.run(model.z_mean, feed_dict=feed_dict)\n",
        "\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # Predict on test set of edges\n",
        "    adj_rec = np.dot(emb, emb.T)\n",
        "    preds = []\n",
        "    pos = []\n",
        "    for e in edges_pos:\n",
        "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "        pos.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "    preds_neg = []\n",
        "    neg = []\n",
        "    for e in edges_neg:\n",
        "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
        "        neg.append(adj_orig[e[0], e[1]])\n",
        "\n",
        "    preds_all = np.hstack([preds, preds_neg])\n",
        "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
        "    roc_score = roc_auc_score(labels_all, preds_all)\n",
        "    ap_score = average_precision_score(labels_all, preds_all)\n",
        "\n",
        "    return roc_score, ap_score"
      ],
      "metadata": {
        "id": "6MZgZ7sHeYTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_val = []\n",
        "acc_val = []\n",
        "val_roc_score = []\n",
        "\n",
        "adj_label = adj + sp.eye(adj.shape[0])\n",
        "adj_label = sparse_to_tuple(adj_label)\n",
        "\n",
        "# Train model\n",
        "for epoch in range(500):\n",
        "\n",
        "    t = time.time()\n",
        "    # Construct feed dictionary\n",
        "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: 0.})\n",
        "    # Run single weight update\n",
        "    outs = sess.run([opt.opt_op, opt.cost, opt.accuracy], feed_dict=feed_dict)\n",
        "\n",
        "    # Compute average loss\n",
        "    avg_cost = outs[1]\n",
        "    avg_accuracy = outs[2]\n",
        "    cost_val.append(avg_cost)\n",
        "    #roc_curr, ap_curr = get_roc_score(val_edges, val_edges_false)\n",
        "    #val_roc_score.append(roc_curr)\n",
        "\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
        "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
        "#roc_score, ap_score = get_roc_score(test_edges, test_edges_false)\n",
        "#print('Test ROC score: ' + str(roc_score))\n",
        "#print('Test AP score: ' + str(ap_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BF3f7oje6dv",
        "outputId": "4cbecf40-8898-4d97-9be1-2ddacac78c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 train_loss= 1.11843 time= 0.09514\n",
            "Epoch: 0002 train_loss= 0.66981 time= 0.01473\n",
            "Epoch: 0003 train_loss= 0.83190 time= 0.01405\n",
            "Epoch: 0004 train_loss= 0.77020 time= 0.01523\n",
            "Epoch: 0005 train_loss= 0.68445 time= 0.01411\n",
            "Epoch: 0006 train_loss= 0.66370 time= 0.01540\n",
            "Epoch: 0007 train_loss= 0.66398 time= 0.01367\n",
            "Epoch: 0008 train_loss= 0.66482 time= 0.01622\n",
            "Epoch: 0009 train_loss= 0.66189 time= 0.01594\n",
            "Epoch: 0010 train_loss= 0.65639 time= 0.01406\n",
            "Epoch: 0011 train_loss= 0.64941 time= 0.01270\n",
            "Epoch: 0012 train_loss= 0.64164 time= 0.01305\n",
            "Epoch: 0013 train_loss= 0.63448 time= 0.01253\n",
            "Epoch: 0014 train_loss= 0.62810 time= 0.01187\n",
            "Epoch: 0015 train_loss= 0.62186 time= 0.01766\n",
            "Epoch: 0016 train_loss= 0.61542 time= 0.01565\n",
            "Epoch: 0017 train_loss= 0.60854 time= 0.01529\n",
            "Epoch: 0018 train_loss= 0.60027 time= 0.01479\n",
            "Epoch: 0019 train_loss= 0.59035 time= 0.01202\n",
            "Epoch: 0020 train_loss= 0.57932 time= 0.01284\n",
            "Epoch: 0021 train_loss= 0.56831 time= 0.01419\n",
            "Epoch: 0022 train_loss= 0.55839 time= 0.01820\n",
            "Epoch: 0023 train_loss= 0.54973 time= 0.01220\n",
            "Epoch: 0024 train_loss= 0.54167 time= 0.01230\n",
            "Epoch: 0025 train_loss= 0.53372 time= 0.01174\n",
            "Epoch: 0026 train_loss= 0.52608 time= 0.01207\n",
            "Epoch: 0027 train_loss= 0.51819 time= 0.01195\n",
            "Epoch: 0028 train_loss= 0.51055 time= 0.01330\n",
            "Epoch: 0029 train_loss= 0.50534 time= 0.01227\n",
            "Epoch: 0030 train_loss= 0.50326 time= 0.01740\n",
            "Epoch: 0031 train_loss= 0.50291 time= 0.01393\n",
            "Epoch: 0032 train_loss= 0.50255 time= 0.01347\n",
            "Epoch: 0033 train_loss= 0.50134 time= 0.01109\n",
            "Epoch: 0034 train_loss= 0.50018 time= 0.01241\n",
            "Epoch: 0035 train_loss= 0.49900 time= 0.01848\n",
            "Epoch: 0036 train_loss= 0.49717 time= 0.01213\n",
            "Epoch: 0037 train_loss= 0.49424 time= 0.01189\n",
            "Epoch: 0038 train_loss= 0.49149 time= 0.01512\n",
            "Epoch: 0039 train_loss= 0.48975 time= 0.01220\n",
            "Epoch: 0040 train_loss= 0.48801 time= 0.01328\n",
            "Epoch: 0041 train_loss= 0.48516 time= 0.01337\n",
            "Epoch: 0042 train_loss= 0.48221 time= 0.01539\n",
            "Epoch: 0043 train_loss= 0.48003 time= 0.01409\n",
            "Epoch: 0044 train_loss= 0.47814 time= 0.01439\n",
            "Epoch: 0045 train_loss= 0.47612 time= 0.01598\n",
            "Epoch: 0046 train_loss= 0.47424 time= 0.01380\n",
            "Epoch: 0047 train_loss= 0.47309 time= 0.01323\n",
            "Epoch: 0048 train_loss= 0.47220 time= 0.01382\n",
            "Epoch: 0049 train_loss= 0.47109 time= 0.01456\n",
            "Epoch: 0050 train_loss= 0.46992 time= 0.02000\n",
            "Epoch: 0051 train_loss= 0.46880 time= 0.01415\n",
            "Epoch: 0052 train_loss= 0.46739 time= 0.01290\n",
            "Epoch: 0053 train_loss= 0.46574 time= 0.02137\n",
            "Epoch: 0054 train_loss= 0.46431 time= 0.01912\n",
            "Epoch: 0055 train_loss= 0.46280 time= 0.01193\n",
            "Epoch: 0056 train_loss= 0.46113 time= 0.01137\n",
            "Epoch: 0057 train_loss= 0.45977 time= 0.01266\n",
            "Epoch: 0058 train_loss= 0.45826 time= 0.01234\n",
            "Epoch: 0059 train_loss= 0.45649 time= 0.01781\n",
            "Epoch: 0060 train_loss= 0.45483 time= 0.01234\n",
            "Epoch: 0061 train_loss= 0.45302 time= 0.01302\n",
            "Epoch: 0062 train_loss= 0.45106 time= 0.01343\n",
            "Epoch: 0063 train_loss= 0.44929 time= 0.01354\n",
            "Epoch: 0064 train_loss= 0.44764 time= 0.01235\n",
            "Epoch: 0065 train_loss= 0.44599 time= 0.02322\n",
            "Epoch: 0066 train_loss= 0.44457 time= 0.01489\n",
            "Epoch: 0067 train_loss= 0.44306 time= 0.01214\n",
            "Epoch: 0068 train_loss= 0.44163 time= 0.01309\n",
            "Epoch: 0069 train_loss= 0.44013 time= 0.01182\n",
            "Epoch: 0070 train_loss= 0.43857 time= 0.01213\n",
            "Epoch: 0071 train_loss= 0.43714 time= 0.01312\n",
            "Epoch: 0072 train_loss= 0.43572 time= 0.01221\n",
            "Epoch: 0073 train_loss= 0.43439 time= 0.01238\n",
            "Epoch: 0074 train_loss= 0.43308 time= 0.01641\n",
            "Epoch: 0075 train_loss= 0.43188 time= 0.01246\n",
            "Epoch: 0076 train_loss= 0.43061 time= 0.01633\n",
            "Epoch: 0077 train_loss= 0.42934 time= 0.01313\n",
            "Epoch: 0078 train_loss= 0.42803 time= 0.01439\n",
            "Epoch: 0079 train_loss= 0.42685 time= 0.01755\n",
            "Epoch: 0080 train_loss= 0.42588 time= 0.01343\n",
            "Epoch: 0081 train_loss= 0.42495 time= 0.01376\n",
            "Epoch: 0082 train_loss= 0.42400 time= 0.01277\n",
            "Epoch: 0083 train_loss= 0.42299 time= 0.01246\n",
            "Epoch: 0084 train_loss= 0.42208 time= 0.01496\n",
            "Epoch: 0085 train_loss= 0.42136 time= 0.01457\n",
            "Epoch: 0086 train_loss= 0.42070 time= 0.01502\n",
            "Epoch: 0087 train_loss= 0.41998 time= 0.01421\n",
            "Epoch: 0088 train_loss= 0.41920 time= 0.01730\n",
            "Epoch: 0089 train_loss= 0.41851 time= 0.01345\n",
            "Epoch: 0090 train_loss= 0.41784 time= 0.01083\n",
            "Epoch: 0091 train_loss= 0.41704 time= 0.01372\n",
            "Epoch: 0092 train_loss= 0.41627 time= 0.01342\n",
            "Epoch: 0093 train_loss= 0.41562 time= 0.01396\n",
            "Epoch: 0094 train_loss= 0.41498 time= 0.01327\n",
            "Epoch: 0095 train_loss= 0.41432 time= 0.01387\n",
            "Epoch: 0096 train_loss= 0.41375 time= 0.01257\n",
            "Epoch: 0097 train_loss= 0.41322 time= 0.01225\n",
            "Epoch: 0098 train_loss= 0.41276 time= 0.01292\n",
            "Epoch: 0099 train_loss= 0.41207 time= 0.01275\n",
            "Epoch: 0100 train_loss= 0.41139 time= 0.01225\n",
            "Epoch: 0101 train_loss= 0.41091 time= 0.01212\n",
            "Epoch: 0102 train_loss= 0.41067 time= 0.01182\n",
            "Epoch: 0103 train_loss= 0.41058 time= 0.01315\n",
            "Epoch: 0104 train_loss= 0.41056 time= 0.01611\n",
            "Epoch: 0105 train_loss= 0.40984 time= 0.01173\n",
            "Epoch: 0106 train_loss= 0.40887 time= 0.01104\n",
            "Epoch: 0107 train_loss= 0.40869 time= 0.01233\n",
            "Epoch: 0108 train_loss= 0.40818 time= 0.01149\n",
            "Epoch: 0109 train_loss= 0.40728 time= 0.01233\n",
            "Epoch: 0110 train_loss= 0.40732 time= 0.01268\n",
            "Epoch: 0111 train_loss= 0.40704 time= 0.01233\n",
            "Epoch: 0112 train_loss= 0.40606 time= 0.01185\n",
            "Epoch: 0113 train_loss= 0.40603 time= 0.01207\n",
            "Epoch: 0114 train_loss= 0.40592 time= 0.01300\n",
            "Epoch: 0115 train_loss= 0.40507 time= 0.01244\n",
            "Epoch: 0116 train_loss= 0.40486 time= 0.01493\n",
            "Epoch: 0117 train_loss= 0.40484 time= 0.01311\n",
            "Epoch: 0118 train_loss= 0.40417 time= 0.01359\n",
            "Epoch: 0119 train_loss= 0.40381 time= 0.01946\n",
            "Epoch: 0120 train_loss= 0.40380 time= 0.01464\n",
            "Epoch: 0121 train_loss= 0.40338 time= 0.01124\n",
            "Epoch: 0122 train_loss= 0.40293 time= 0.01290\n",
            "Epoch: 0123 train_loss= 0.40283 time= 0.01347\n",
            "Epoch: 0124 train_loss= 0.40262 time= 0.01590\n",
            "Epoch: 0125 train_loss= 0.40218 time= 0.01128\n",
            "Epoch: 0126 train_loss= 0.40192 time= 0.01226\n",
            "Epoch: 0127 train_loss= 0.40180 time= 0.01229\n",
            "Epoch: 0128 train_loss= 0.40152 time= 0.01314\n",
            "Epoch: 0129 train_loss= 0.40115 time= 0.01483\n",
            "Epoch: 0130 train_loss= 0.40094 time= 0.01548\n",
            "Epoch: 0131 train_loss= 0.40080 time= 0.01342\n",
            "Epoch: 0132 train_loss= 0.40052 time= 0.01360\n",
            "Epoch: 0133 train_loss= 0.40021 time= 0.01283\n",
            "Epoch: 0134 train_loss= 0.40001 time= 0.01636\n",
            "Epoch: 0135 train_loss= 0.39983 time= 0.01446\n",
            "Epoch: 0136 train_loss= 0.39958 time= 0.01128\n",
            "Epoch: 0137 train_loss= 0.39931 time= 0.02137\n",
            "Epoch: 0138 train_loss= 0.39908 time= 0.01579\n",
            "Epoch: 0139 train_loss= 0.39890 time= 0.01377\n",
            "Epoch: 0140 train_loss= 0.39871 time= 0.01391\n",
            "Epoch: 0141 train_loss= 0.39847 time= 0.01354\n",
            "Epoch: 0142 train_loss= 0.39820 time= 0.01350\n",
            "Epoch: 0143 train_loss= 0.39796 time= 0.01274\n",
            "Epoch: 0144 train_loss= 0.39776 time= 0.01215\n",
            "Epoch: 0145 train_loss= 0.39756 time= 0.01370\n",
            "Epoch: 0146 train_loss= 0.39734 time= 0.01433\n",
            "Epoch: 0147 train_loss= 0.39710 time= 0.01339\n",
            "Epoch: 0148 train_loss= 0.39688 time= 0.01331\n",
            "Epoch: 0149 train_loss= 0.39667 time= 0.01547\n",
            "Epoch: 0150 train_loss= 0.39649 time= 0.01131\n",
            "Epoch: 0151 train_loss= 0.39630 time= 0.01361\n",
            "Epoch: 0152 train_loss= 0.39612 time= 0.01246\n",
            "Epoch: 0153 train_loss= 0.39597 time= 0.01305\n",
            "Epoch: 0154 train_loss= 0.39593 time= 0.01283\n",
            "Epoch: 0155 train_loss= 0.39605 time= 0.01426\n",
            "Epoch: 0156 train_loss= 0.39646 time= 0.01395\n",
            "Epoch: 0157 train_loss= 0.39664 time= 0.01213\n",
            "Epoch: 0158 train_loss= 0.39629 time= 0.01449\n",
            "Epoch: 0159 train_loss= 0.39509 time= 0.01445\n",
            "Epoch: 0160 train_loss= 0.39456 time= 0.01359\n",
            "Epoch: 0161 train_loss= 0.39484 time= 0.02123\n",
            "Epoch: 0162 train_loss= 0.39464 time= 0.01196\n",
            "Epoch: 0163 train_loss= 0.39389 time= 0.01796\n",
            "Epoch: 0164 train_loss= 0.39365 time= 0.01247\n",
            "Epoch: 0165 train_loss= 0.39401 time= 0.01206\n",
            "Epoch: 0166 train_loss= 0.39386 time= 0.01338\n",
            "Epoch: 0167 train_loss= 0.39316 time= 0.01483\n",
            "Epoch: 0168 train_loss= 0.39288 time= 0.01274\n",
            "Epoch: 0169 train_loss= 0.39295 time= 0.01349\n",
            "Epoch: 0170 train_loss= 0.39261 time= 0.01259\n",
            "Epoch: 0171 train_loss= 0.39212 time= 0.01452\n",
            "Epoch: 0172 train_loss= 0.39200 time= 0.01480\n",
            "Epoch: 0173 train_loss= 0.39206 time= 0.01140\n",
            "Epoch: 0174 train_loss= 0.39184 time= 0.01265\n",
            "Epoch: 0175 train_loss= 0.39141 time= 0.01247\n",
            "Epoch: 0176 train_loss= 0.39128 time= 0.01365\n",
            "Epoch: 0177 train_loss= 0.39130 time= 0.01226\n",
            "Epoch: 0178 train_loss= 0.39112 time= 0.01622\n",
            "Epoch: 0179 train_loss= 0.39074 time= 0.01220\n",
            "Epoch: 0180 train_loss= 0.39045 time= 0.01354\n",
            "Epoch: 0181 train_loss= 0.39026 time= 0.01242\n",
            "Epoch: 0182 train_loss= 0.38997 time= 0.01213\n",
            "Epoch: 0183 train_loss= 0.38954 time= 0.01298\n",
            "Epoch: 0184 train_loss= 0.38924 time= 0.01422\n",
            "Epoch: 0185 train_loss= 0.38914 time= 0.01356\n",
            "Epoch: 0186 train_loss= 0.38905 time= 0.01536\n",
            "Epoch: 0187 train_loss= 0.38883 time= 0.01235\n",
            "Epoch: 0188 train_loss= 0.38851 time= 0.01174\n",
            "Epoch: 0189 train_loss= 0.38829 time= 0.01786\n",
            "Epoch: 0190 train_loss= 0.38809 time= 0.01242\n",
            "Epoch: 0191 train_loss= 0.38785 time= 0.01306\n",
            "Epoch: 0192 train_loss= 0.38754 time= 0.01391\n",
            "Epoch: 0193 train_loss= 0.38719 time= 0.01814\n",
            "Epoch: 0194 train_loss= 0.38691 time= 0.01442\n",
            "Epoch: 0195 train_loss= 0.38670 time= 0.01363\n",
            "Epoch: 0196 train_loss= 0.38650 time= 0.01307\n",
            "Epoch: 0197 train_loss= 0.38630 time= 0.01183\n",
            "Epoch: 0198 train_loss= 0.38606 time= 0.01145\n",
            "Epoch: 0199 train_loss= 0.38583 time= 0.01207\n",
            "Epoch: 0200 train_loss= 0.38564 time= 0.01342\n",
            "Epoch: 0201 train_loss= 0.38547 time= 0.01373\n",
            "Epoch: 0202 train_loss= 0.38536 time= 0.01446\n",
            "Epoch: 0203 train_loss= 0.38518 time= 0.01538\n",
            "Epoch: 0204 train_loss= 0.38491 time= 0.01432\n",
            "Epoch: 0205 train_loss= 0.38452 time= 0.01374\n",
            "Epoch: 0206 train_loss= 0.38414 time= 0.01642\n",
            "Epoch: 0207 train_loss= 0.38383 time= 0.01416\n",
            "Epoch: 0208 train_loss= 0.38363 time= 0.02531\n",
            "Epoch: 0209 train_loss= 0.38350 time= 0.01536\n",
            "Epoch: 0210 train_loss= 0.38336 time= 0.01285\n",
            "Epoch: 0211 train_loss= 0.38322 time= 0.01241\n",
            "Epoch: 0212 train_loss= 0.38300 time= 0.01214\n",
            "Epoch: 0213 train_loss= 0.38280 time= 0.01438\n",
            "Epoch: 0214 train_loss= 0.38251 time= 0.01341\n",
            "Epoch: 0215 train_loss= 0.38221 time= 0.01346\n",
            "Epoch: 0216 train_loss= 0.38186 time= 0.02578\n",
            "Epoch: 0217 train_loss= 0.38153 time= 0.02167\n",
            "Epoch: 0218 train_loss= 0.38123 time= 0.01145\n",
            "Epoch: 0219 train_loss= 0.38098 time= 0.01461\n",
            "Epoch: 0220 train_loss= 0.38076 time= 0.01265\n",
            "Epoch: 0221 train_loss= 0.38056 time= 0.01498\n",
            "Epoch: 0222 train_loss= 0.38043 time= 0.01597\n",
            "Epoch: 0223 train_loss= 0.38039 time= 0.01434\n",
            "Epoch: 0224 train_loss= 0.38056 time= 0.01324\n",
            "Epoch: 0225 train_loss= 0.38091 time= 0.01352\n",
            "Epoch: 0226 train_loss= 0.38158 time= 0.01277\n",
            "Epoch: 0227 train_loss= 0.38135 time= 0.01799\n",
            "Epoch: 0228 train_loss= 0.38060 time= 0.01350\n",
            "Epoch: 0229 train_loss= 0.37948 time= 0.01439\n",
            "Epoch: 0230 train_loss= 0.37954 time= 0.01526\n",
            "Epoch: 0231 train_loss= 0.37965 time= 0.01305\n",
            "Epoch: 0232 train_loss= 0.37859 time= 0.01272\n",
            "Epoch: 0233 train_loss= 0.37762 time= 0.01250\n",
            "Epoch: 0234 train_loss= 0.37785 time= 0.01485\n",
            "Epoch: 0235 train_loss= 0.37827 time= 0.01675\n",
            "Epoch: 0236 train_loss= 0.37779 time= 0.01357\n",
            "Epoch: 0237 train_loss= 0.37675 time= 0.01363\n",
            "Epoch: 0238 train_loss= 0.37634 time= 0.01359\n",
            "Epoch: 0239 train_loss= 0.37660 time= 0.01161\n",
            "Epoch: 0240 train_loss= 0.37667 time= 0.01593\n",
            "Epoch: 0241 train_loss= 0.37619 time= 0.01341\n",
            "Epoch: 0242 train_loss= 0.37542 time= 0.01388\n",
            "Epoch: 0243 train_loss= 0.37507 time= 0.01286\n",
            "Epoch: 0244 train_loss= 0.37514 time= 0.01393\n",
            "Epoch: 0245 train_loss= 0.37508 time= 0.01374\n",
            "Epoch: 0246 train_loss= 0.37465 time= 0.01566\n",
            "Epoch: 0247 train_loss= 0.37409 time= 0.01359\n",
            "Epoch: 0248 train_loss= 0.37383 time= 0.01367\n",
            "Epoch: 0249 train_loss= 0.37380 time= 0.02080\n",
            "Epoch: 0250 train_loss= 0.37372 time= 0.01455\n",
            "Epoch: 0251 train_loss= 0.37349 time= 0.01363\n",
            "Epoch: 0252 train_loss= 0.37343 time= 0.01385\n",
            "Epoch: 0253 train_loss= 0.37405 time= 0.01271\n",
            "Epoch: 0254 train_loss= 0.37712 time= 0.01238\n",
            "Epoch: 0255 train_loss= 0.38212 time= 0.01589\n",
            "Epoch: 0256 train_loss= 0.39108 time= 0.01269\n",
            "Epoch: 0257 train_loss= 0.37869 time= 0.01198\n",
            "Epoch: 0258 train_loss= 0.37272 time= 0.01228\n",
            "Epoch: 0259 train_loss= 0.38016 time= 0.01274\n",
            "Epoch: 0260 train_loss= 0.37444 time= 0.01281\n",
            "Epoch: 0261 train_loss= 0.37329 time= 0.01295\n",
            "Epoch: 0262 train_loss= 0.37701 time= 0.01472\n",
            "Epoch: 0263 train_loss= 0.37145 time= 0.01211\n",
            "Epoch: 0264 train_loss= 0.37464 time= 0.01160\n",
            "Epoch: 0265 train_loss= 0.37441 time= 0.01581\n",
            "Epoch: 0266 train_loss= 0.37083 time= 0.01301\n",
            "Epoch: 0267 train_loss= 0.37485 time= 0.01434\n",
            "Epoch: 0268 train_loss= 0.37243 time= 0.02356\n",
            "Epoch: 0269 train_loss= 0.37097 time= 0.01255\n",
            "Epoch: 0270 train_loss= 0.37355 time= 0.01280\n",
            "Epoch: 0271 train_loss= 0.37078 time= 0.01220\n",
            "Epoch: 0272 train_loss= 0.37082 time= 0.01225\n",
            "Epoch: 0273 train_loss= 0.37157 time= 0.01230\n",
            "Epoch: 0274 train_loss= 0.36954 time= 0.01201\n",
            "Epoch: 0275 train_loss= 0.37035 time= 0.01408\n",
            "Epoch: 0276 train_loss= 0.36962 time= 0.01668\n",
            "Epoch: 0277 train_loss= 0.36893 time= 0.02000\n",
            "Epoch: 0278 train_loss= 0.36959 time= 0.01322\n",
            "Epoch: 0279 train_loss= 0.36819 time= 0.01534\n",
            "Epoch: 0280 train_loss= 0.36867 time= 0.01366\n",
            "Epoch: 0281 train_loss= 0.36864 time= 0.01327\n",
            "Epoch: 0282 train_loss= 0.36741 time= 0.01235\n",
            "Epoch: 0283 train_loss= 0.36831 time= 0.01349\n",
            "Epoch: 0284 train_loss= 0.36770 time= 0.01414\n",
            "Epoch: 0285 train_loss= 0.36692 time= 0.01327\n",
            "Epoch: 0286 train_loss= 0.36763 time= 0.01535\n",
            "Epoch: 0287 train_loss= 0.36693 time= 0.01481\n",
            "Epoch: 0288 train_loss= 0.36641 time= 0.01195\n",
            "Epoch: 0289 train_loss= 0.36683 time= 0.01351\n",
            "Epoch: 0290 train_loss= 0.36637 time= 0.01301\n",
            "Epoch: 0291 train_loss= 0.36578 time= 0.01407\n",
            "Epoch: 0292 train_loss= 0.36613 time= 0.01209\n",
            "Epoch: 0293 train_loss= 0.36580 time= 0.01594\n",
            "Epoch: 0294 train_loss= 0.36516 time= 0.01720\n",
            "Epoch: 0295 train_loss= 0.36545 time= 0.01392\n",
            "Epoch: 0296 train_loss= 0.36524 time= 0.01297\n",
            "Epoch: 0297 train_loss= 0.36460 time= 0.01170\n",
            "Epoch: 0298 train_loss= 0.36477 time= 0.01253\n",
            "Epoch: 0299 train_loss= 0.36467 time= 0.01238\n",
            "Epoch: 0300 train_loss= 0.36414 time= 0.01430\n",
            "Epoch: 0301 train_loss= 0.36404 time= 0.01332\n",
            "Epoch: 0302 train_loss= 0.36405 time= 0.01468\n",
            "Epoch: 0303 train_loss= 0.36370 time= 0.01458\n",
            "Epoch: 0304 train_loss= 0.36338 time= 0.01279\n",
            "Epoch: 0305 train_loss= 0.36336 time= 0.01395\n",
            "Epoch: 0306 train_loss= 0.36324 time= 0.01440\n",
            "Epoch: 0307 train_loss= 0.36286 time= 0.01293\n",
            "Epoch: 0308 train_loss= 0.36268 time= 0.01372\n",
            "Epoch: 0309 train_loss= 0.36265 time= 0.01307\n",
            "Epoch: 0310 train_loss= 0.36242 time= 0.01163\n",
            "Epoch: 0311 train_loss= 0.36215 time= 0.01171\n",
            "Epoch: 0312 train_loss= 0.36200 time= 0.01235\n",
            "Epoch: 0313 train_loss= 0.36190 time= 0.01192\n",
            "Epoch: 0314 train_loss= 0.36172 time= 0.01212\n",
            "Epoch: 0315 train_loss= 0.36150 time= 0.01220\n",
            "Epoch: 0316 train_loss= 0.36142 time= 0.01217\n",
            "Epoch: 0317 train_loss= 0.36153 time= 0.01829\n",
            "Epoch: 0318 train_loss= 0.36193 time= 0.01328\n",
            "Epoch: 0319 train_loss= 0.36309 time= 0.01288\n",
            "Epoch: 0320 train_loss= 0.36651 time= 0.01386\n",
            "Epoch: 0321 train_loss= 0.37336 time= 0.01194\n",
            "Epoch: 0322 train_loss= 0.38329 time= 0.01495\n",
            "Epoch: 0323 train_loss= 0.38019 time= 0.01426\n",
            "Epoch: 0324 train_loss= 0.36552 time= 0.01226\n",
            "Epoch: 0325 train_loss= 0.36295 time= 0.01542\n",
            "Epoch: 0326 train_loss= 0.37314 time= 0.01389\n",
            "Epoch: 0327 train_loss= 0.36793 time= 0.01055\n",
            "Epoch: 0328 train_loss= 0.36218 time= 0.01313\n",
            "Epoch: 0329 train_loss= 0.36977 time= 0.01349\n",
            "Epoch: 0330 train_loss= 0.36576 time= 0.01333\n",
            "Epoch: 0331 train_loss= 0.36319 time= 0.01238\n",
            "Epoch: 0332 train_loss= 0.36752 time= 0.01212\n",
            "Epoch: 0333 train_loss= 0.36379 time= 0.01390\n",
            "Epoch: 0334 train_loss= 0.36371 time= 0.01340\n",
            "Epoch: 0335 train_loss= 0.36463 time= 0.01303\n",
            "Epoch: 0336 train_loss= 0.36265 time= 0.01441\n",
            "Epoch: 0337 train_loss= 0.36271 time= 0.01648\n",
            "Epoch: 0338 train_loss= 0.36227 time= 0.01315\n",
            "Epoch: 0339 train_loss= 0.36205 time= 0.01348\n",
            "Epoch: 0340 train_loss= 0.36077 time= 0.01207\n",
            "Epoch: 0341 train_loss= 0.36118 time= 0.01400\n",
            "Epoch: 0342 train_loss= 0.36111 time= 0.01261\n",
            "Epoch: 0343 train_loss= 0.35919 time= 0.01255\n",
            "Epoch: 0344 train_loss= 0.36095 time= 0.01269\n",
            "Epoch: 0345 train_loss= 0.35975 time= 0.01342\n",
            "Epoch: 0346 train_loss= 0.35851 time= 0.01461\n",
            "Epoch: 0347 train_loss= 0.36032 time= 0.02099\n",
            "Epoch: 0348 train_loss= 0.35830 time= 0.01417\n",
            "Epoch: 0349 train_loss= 0.35819 time= 0.01195\n",
            "Epoch: 0350 train_loss= 0.35899 time= 0.01449\n",
            "Epoch: 0351 train_loss= 0.35723 time= 0.01583\n",
            "Epoch: 0352 train_loss= 0.35782 time= 0.01191\n",
            "Epoch: 0353 train_loss= 0.35758 time= 0.01242\n",
            "Epoch: 0354 train_loss= 0.35668 time= 0.01379\n",
            "Epoch: 0355 train_loss= 0.35730 time= 0.01367\n",
            "Epoch: 0356 train_loss= 0.35649 time= 0.01291\n",
            "Epoch: 0357 train_loss= 0.35639 time= 0.01143\n",
            "Epoch: 0358 train_loss= 0.35671 time= 0.01477\n",
            "Epoch: 0359 train_loss= 0.35576 time= 0.01375\n",
            "Epoch: 0360 train_loss= 0.35606 time= 0.01590\n",
            "Epoch: 0361 train_loss= 0.35613 time= 0.01423\n",
            "Epoch: 0362 train_loss= 0.35529 time= 0.01470\n",
            "Epoch: 0363 train_loss= 0.35559 time= 0.01517\n",
            "Epoch: 0364 train_loss= 0.35554 time= 0.01402\n",
            "Epoch: 0365 train_loss= 0.35496 time= 0.01481\n",
            "Epoch: 0366 train_loss= 0.35515 time= 0.01638\n",
            "Epoch: 0367 train_loss= 0.35490 time= 0.01293\n",
            "Epoch: 0368 train_loss= 0.35453 time= 0.01365\n",
            "Epoch: 0369 train_loss= 0.35473 time= 0.01432\n",
            "Epoch: 0370 train_loss= 0.35443 time= 0.01476\n",
            "Epoch: 0371 train_loss= 0.35403 time= 0.01257\n",
            "Epoch: 0372 train_loss= 0.35421 time= 0.01086\n",
            "Epoch: 0373 train_loss= 0.35406 time= 0.01935\n",
            "Epoch: 0374 train_loss= 0.35369 time= 0.01322\n",
            "Epoch: 0375 train_loss= 0.35374 time= 0.01296\n",
            "Epoch: 0376 train_loss= 0.35359 time= 0.01318\n",
            "Epoch: 0377 train_loss= 0.35324 time= 0.01323\n",
            "Epoch: 0378 train_loss= 0.35325 time= 0.01466\n",
            "Epoch: 0379 train_loss= 0.35322 time= 0.01279\n",
            "Epoch: 0380 train_loss= 0.35293 time= 0.01355\n",
            "Epoch: 0381 train_loss= 0.35284 time= 0.01456\n",
            "Epoch: 0382 train_loss= 0.35284 time= 0.01347\n",
            "Epoch: 0383 train_loss= 0.35259 time= 0.01350\n",
            "Epoch: 0384 train_loss= 0.35242 time= 0.01293\n",
            "Epoch: 0385 train_loss= 0.35239 time= 0.01394\n",
            "Epoch: 0386 train_loss= 0.35224 time= 0.01232\n",
            "Epoch: 0387 train_loss= 0.35204 time= 0.01407\n",
            "Epoch: 0388 train_loss= 0.35196 time= 0.01323\n",
            "Epoch: 0389 train_loss= 0.35190 time= 0.01240\n",
            "Epoch: 0390 train_loss= 0.35178 time= 0.01328\n",
            "Epoch: 0391 train_loss= 0.35170 time= 0.01240\n",
            "Epoch: 0392 train_loss= 0.35180 time= 0.01386\n",
            "Epoch: 0393 train_loss= 0.35215 time= 0.01534\n",
            "Epoch: 0394 train_loss= 0.35307 time= 0.01276\n",
            "Epoch: 0395 train_loss= 0.35525 time= 0.01351\n",
            "Epoch: 0396 train_loss= 0.36109 time= 0.01353\n",
            "Epoch: 0397 train_loss= 0.36943 time= 0.01260\n",
            "Epoch: 0398 train_loss= 0.38497 time= 0.01229\n",
            "Epoch: 0399 train_loss= 0.38150 time= 0.01374\n",
            "Epoch: 0400 train_loss= 0.35975 time= 0.01207\n",
            "Epoch: 0401 train_loss= 0.35336 time= 0.01284\n",
            "Epoch: 0402 train_loss= 0.36736 time= 0.01312\n",
            "Epoch: 0403 train_loss= 0.35981 time= 0.01064\n",
            "Epoch: 0404 train_loss= 0.35254 time= 0.01326\n",
            "Epoch: 0405 train_loss= 0.36310 time= 0.01364\n",
            "Epoch: 0406 train_loss= 0.35522 time= 0.01240\n",
            "Epoch: 0407 train_loss= 0.35422 time= 0.01515\n",
            "Epoch: 0408 train_loss= 0.35955 time= 0.02037\n",
            "Epoch: 0409 train_loss= 0.35152 time= 0.01756\n",
            "Epoch: 0410 train_loss= 0.35586 time= 0.01466\n",
            "Epoch: 0411 train_loss= 0.35444 time= 0.01310\n",
            "Epoch: 0412 train_loss= 0.35178 time= 0.01284\n",
            "Epoch: 0413 train_loss= 0.35514 time= 0.01334\n",
            "Epoch: 0414 train_loss= 0.35170 time= 0.01468\n",
            "Epoch: 0415 train_loss= 0.35351 time= 0.01249\n",
            "Epoch: 0416 train_loss= 0.35263 time= 0.01596\n",
            "Epoch: 0417 train_loss= 0.35167 time= 0.01834\n",
            "Epoch: 0418 train_loss= 0.35290 time= 0.01453\n",
            "Epoch: 0419 train_loss= 0.35066 time= 0.01152\n",
            "Epoch: 0420 train_loss= 0.35189 time= 0.01198\n",
            "Epoch: 0421 train_loss= 0.35095 time= 0.01238\n",
            "Epoch: 0422 train_loss= 0.35031 time= 0.01308\n",
            "Epoch: 0423 train_loss= 0.35144 time= 0.01562\n",
            "Epoch: 0424 train_loss= 0.34972 time= 0.01177\n",
            "Epoch: 0425 train_loss= 0.35052 time= 0.01280\n",
            "Epoch: 0426 train_loss= 0.35065 time= 0.01248\n",
            "Epoch: 0427 train_loss= 0.34936 time= 0.01483\n",
            "Epoch: 0428 train_loss= 0.35044 time= 0.01555\n",
            "Epoch: 0429 train_loss= 0.34993 time= 0.01696\n",
            "Epoch: 0430 train_loss= 0.34925 time= 0.01523\n",
            "Epoch: 0431 train_loss= 0.35000 time= 0.01178\n",
            "Epoch: 0432 train_loss= 0.34934 time= 0.01477\n",
            "Epoch: 0433 train_loss= 0.34911 time= 0.01374\n",
            "Epoch: 0434 train_loss= 0.34946 time= 0.01279\n",
            "Epoch: 0435 train_loss= 0.34886 time= 0.01132\n",
            "Epoch: 0436 train_loss= 0.34886 time= 0.01497\n",
            "Epoch: 0437 train_loss= 0.34901 time= 0.01582\n",
            "Epoch: 0438 train_loss= 0.34847 time= 0.01468\n",
            "Epoch: 0439 train_loss= 0.34850 time= 0.01529\n",
            "Epoch: 0440 train_loss= 0.34866 time= 0.01445\n",
            "Epoch: 0441 train_loss= 0.34819 time= 0.01266\n",
            "Epoch: 0442 train_loss= 0.34809 time= 0.01366\n",
            "Epoch: 0443 train_loss= 0.34833 time= 0.01309\n",
            "Epoch: 0444 train_loss= 0.34803 time= 0.01424\n",
            "Epoch: 0445 train_loss= 0.34775 time= 0.02066\n",
            "Epoch: 0446 train_loss= 0.34792 time= 0.01276\n",
            "Epoch: 0447 train_loss= 0.34786 time= 0.01339\n",
            "Epoch: 0448 train_loss= 0.34755 time= 0.01348\n",
            "Epoch: 0449 train_loss= 0.34753 time= 0.01814\n",
            "Epoch: 0450 train_loss= 0.34758 time= 0.01217\n",
            "Epoch: 0451 train_loss= 0.34739 time= 0.01685\n",
            "Epoch: 0452 train_loss= 0.34726 time= 0.01218\n",
            "Epoch: 0453 train_loss= 0.34726 time= 0.01225\n",
            "Epoch: 0454 train_loss= 0.34716 time= 0.01205\n",
            "Epoch: 0455 train_loss= 0.34703 time= 0.01209\n",
            "Epoch: 0456 train_loss= 0.34699 time= 0.01365\n",
            "Epoch: 0457 train_loss= 0.34694 time= 0.01141\n",
            "Epoch: 0458 train_loss= 0.34681 time= 0.01307\n",
            "Epoch: 0459 train_loss= 0.34669 time= 0.01202\n",
            "Epoch: 0460 train_loss= 0.34667 time= 0.01322\n",
            "Epoch: 0461 train_loss= 0.34664 time= 0.01397\n",
            "Epoch: 0462 train_loss= 0.34653 time= 0.01252\n",
            "Epoch: 0463 train_loss= 0.34640 time= 0.01234\n",
            "Epoch: 0464 train_loss= 0.34634 time= 0.01289\n",
            "Epoch: 0465 train_loss= 0.34630 time= 0.01331\n",
            "Epoch: 0466 train_loss= 0.34624 time= 0.01447\n",
            "Epoch: 0467 train_loss= 0.34615 time= 0.01709\n",
            "Epoch: 0468 train_loss= 0.34606 time= 0.01274\n",
            "Epoch: 0469 train_loss= 0.34600 time= 0.01430\n",
            "Epoch: 0470 train_loss= 0.34595 time= 0.01415\n",
            "Epoch: 0471 train_loss= 0.34588 time= 0.01568\n",
            "Epoch: 0472 train_loss= 0.34579 time= 0.01376\n",
            "Epoch: 0473 train_loss= 0.34571 time= 0.01391\n",
            "Epoch: 0474 train_loss= 0.34566 time= 0.01923\n",
            "Epoch: 0475 train_loss= 0.34561 time= 0.01199\n",
            "Epoch: 0476 train_loss= 0.34555 time= 0.01279\n",
            "Epoch: 0477 train_loss= 0.34549 time= 0.01524\n",
            "Epoch: 0478 train_loss= 0.34542 time= 0.01291\n",
            "Epoch: 0479 train_loss= 0.34536 time= 0.01298\n",
            "Epoch: 0480 train_loss= 0.34530 time= 0.01475\n",
            "Epoch: 0481 train_loss= 0.34525 time= 0.01695\n",
            "Epoch: 0482 train_loss= 0.34520 time= 0.01353\n",
            "Epoch: 0483 train_loss= 0.34516 time= 0.01316\n",
            "Epoch: 0484 train_loss= 0.34515 time= 0.01435\n",
            "Epoch: 0485 train_loss= 0.34517 time= 0.01268\n",
            "Epoch: 0486 train_loss= 0.34528 time= 0.01424\n",
            "Epoch: 0487 train_loss= 0.34552 time= 0.01452\n",
            "Epoch: 0488 train_loss= 0.34603 time= 0.02048\n",
            "Epoch: 0489 train_loss= 0.34686 time= 0.01407\n",
            "Epoch: 0490 train_loss= 0.34857 time= 0.01309\n",
            "Epoch: 0491 train_loss= 0.35079 time= 0.01333\n",
            "Epoch: 0492 train_loss= 0.35443 time= 0.01356\n",
            "Epoch: 0493 train_loss= 0.35634 time= 0.01292\n",
            "Epoch: 0494 train_loss= 0.35737 time= 0.01494\n",
            "Epoch: 0495 train_loss= 0.35077 time= 0.01717\n",
            "Epoch: 0496 train_loss= 0.34545 time= 0.01243\n",
            "Epoch: 0497 train_loss= 0.34546 time= 0.01320\n",
            "Epoch: 0498 train_loss= 0.34917 time= 0.01392\n",
            "Epoch: 0499 train_loss= 0.35015 time= 0.01496\n",
            "Epoch: 0500 train_loss= 0.34616 time= 0.01467\n",
            "Optimization Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoyuRwXmkWv",
        "outputId": "b596a042-d1a2-4bd1-a565-a157fac2bded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(438, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(emb).to_csv(\"/content/drive/MyDrive/Thesis_5_1/data/GAE-128-8-new.csv\")"
      ],
      "metadata": {
        "id": "brRRfmpVo_P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bb-5OyfdDFwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
